{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN - InceptionV3.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lcroffi/CNN/blob/master/CNN_InceptionV3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZv6wAwwUJ2T",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "30d30105-ff8a-4648-aeb6-a2fc4a1176c1"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import keras\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.layers import Dense, GlobalAveragePooling2D, Dropout, Flatten\n",
        "from keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
        "from keras.preprocessing import image\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Model\n",
        "from keras.optimizers import Adam"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ErFm8QYU1VV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "img_width, img_height = 299, 299"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gpBFJPmvVD96",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 360
        },
        "outputId": "a7fcf0e9-ba0e-46ec-9779-de15910f3fac"
      },
      "source": [
        "# importa o modelo InceptionV3 e descarta a Ãºltima camada do classifier.\n",
        "base_model=InceptionV3(weights='imagenet',include_top=False, input_shape=(img_width, img_height, 3))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0717 16:29:30.877536 140452496836480 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0717 16:29:30.920881 140452496836480 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0717 16:29:30.928847 140452496836480 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0717 16:29:30.966388 140452496836480 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "W0717 16:29:30.967934 140452496836480 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "W0717 16:29:33.621023 140452496836480 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n",
            "W0717 16:29:33.905800 140452496836480 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "W0717 16:29:34.763677 140452496836480 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3980: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.5/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "87916544/87910968 [==============================] - 18s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gyQ_sPL-VXkE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "31c954fa-48db-4d0d-a25f-931822564f7c"
      },
      "source": [
        "for i, layer in enumerate(base_model.layers):\n",
        "   print(i, layer.name)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 input_2\n",
            "1 conv2d_95\n",
            "2 batch_normalization_95\n",
            "3 activation_95\n",
            "4 conv2d_96\n",
            "5 batch_normalization_96\n",
            "6 activation_96\n",
            "7 conv2d_97\n",
            "8 batch_normalization_97\n",
            "9 activation_97\n",
            "10 max_pooling2d_5\n",
            "11 conv2d_98\n",
            "12 batch_normalization_98\n",
            "13 activation_98\n",
            "14 conv2d_99\n",
            "15 batch_normalization_99\n",
            "16 activation_99\n",
            "17 max_pooling2d_6\n",
            "18 conv2d_103\n",
            "19 batch_normalization_103\n",
            "20 activation_103\n",
            "21 conv2d_101\n",
            "22 conv2d_104\n",
            "23 batch_normalization_101\n",
            "24 batch_normalization_104\n",
            "25 activation_101\n",
            "26 activation_104\n",
            "27 average_pooling2d_10\n",
            "28 conv2d_100\n",
            "29 conv2d_102\n",
            "30 conv2d_105\n",
            "31 conv2d_106\n",
            "32 batch_normalization_100\n",
            "33 batch_normalization_102\n",
            "34 batch_normalization_105\n",
            "35 batch_normalization_106\n",
            "36 activation_100\n",
            "37 activation_102\n",
            "38 activation_105\n",
            "39 activation_106\n",
            "40 mixed0\n",
            "41 conv2d_110\n",
            "42 batch_normalization_110\n",
            "43 activation_110\n",
            "44 conv2d_108\n",
            "45 conv2d_111\n",
            "46 batch_normalization_108\n",
            "47 batch_normalization_111\n",
            "48 activation_108\n",
            "49 activation_111\n",
            "50 average_pooling2d_11\n",
            "51 conv2d_107\n",
            "52 conv2d_109\n",
            "53 conv2d_112\n",
            "54 conv2d_113\n",
            "55 batch_normalization_107\n",
            "56 batch_normalization_109\n",
            "57 batch_normalization_112\n",
            "58 batch_normalization_113\n",
            "59 activation_107\n",
            "60 activation_109\n",
            "61 activation_112\n",
            "62 activation_113\n",
            "63 mixed1\n",
            "64 conv2d_117\n",
            "65 batch_normalization_117\n",
            "66 activation_117\n",
            "67 conv2d_115\n",
            "68 conv2d_118\n",
            "69 batch_normalization_115\n",
            "70 batch_normalization_118\n",
            "71 activation_115\n",
            "72 activation_118\n",
            "73 average_pooling2d_12\n",
            "74 conv2d_114\n",
            "75 conv2d_116\n",
            "76 conv2d_119\n",
            "77 conv2d_120\n",
            "78 batch_normalization_114\n",
            "79 batch_normalization_116\n",
            "80 batch_normalization_119\n",
            "81 batch_normalization_120\n",
            "82 activation_114\n",
            "83 activation_116\n",
            "84 activation_119\n",
            "85 activation_120\n",
            "86 mixed2\n",
            "87 conv2d_122\n",
            "88 batch_normalization_122\n",
            "89 activation_122\n",
            "90 conv2d_123\n",
            "91 batch_normalization_123\n",
            "92 activation_123\n",
            "93 conv2d_121\n",
            "94 conv2d_124\n",
            "95 batch_normalization_121\n",
            "96 batch_normalization_124\n",
            "97 activation_121\n",
            "98 activation_124\n",
            "99 max_pooling2d_7\n",
            "100 mixed3\n",
            "101 conv2d_129\n",
            "102 batch_normalization_129\n",
            "103 activation_129\n",
            "104 conv2d_130\n",
            "105 batch_normalization_130\n",
            "106 activation_130\n",
            "107 conv2d_126\n",
            "108 conv2d_131\n",
            "109 batch_normalization_126\n",
            "110 batch_normalization_131\n",
            "111 activation_126\n",
            "112 activation_131\n",
            "113 conv2d_127\n",
            "114 conv2d_132\n",
            "115 batch_normalization_127\n",
            "116 batch_normalization_132\n",
            "117 activation_127\n",
            "118 activation_132\n",
            "119 average_pooling2d_13\n",
            "120 conv2d_125\n",
            "121 conv2d_128\n",
            "122 conv2d_133\n",
            "123 conv2d_134\n",
            "124 batch_normalization_125\n",
            "125 batch_normalization_128\n",
            "126 batch_normalization_133\n",
            "127 batch_normalization_134\n",
            "128 activation_125\n",
            "129 activation_128\n",
            "130 activation_133\n",
            "131 activation_134\n",
            "132 mixed4\n",
            "133 conv2d_139\n",
            "134 batch_normalization_139\n",
            "135 activation_139\n",
            "136 conv2d_140\n",
            "137 batch_normalization_140\n",
            "138 activation_140\n",
            "139 conv2d_136\n",
            "140 conv2d_141\n",
            "141 batch_normalization_136\n",
            "142 batch_normalization_141\n",
            "143 activation_136\n",
            "144 activation_141\n",
            "145 conv2d_137\n",
            "146 conv2d_142\n",
            "147 batch_normalization_137\n",
            "148 batch_normalization_142\n",
            "149 activation_137\n",
            "150 activation_142\n",
            "151 average_pooling2d_14\n",
            "152 conv2d_135\n",
            "153 conv2d_138\n",
            "154 conv2d_143\n",
            "155 conv2d_144\n",
            "156 batch_normalization_135\n",
            "157 batch_normalization_138\n",
            "158 batch_normalization_143\n",
            "159 batch_normalization_144\n",
            "160 activation_135\n",
            "161 activation_138\n",
            "162 activation_143\n",
            "163 activation_144\n",
            "164 mixed5\n",
            "165 conv2d_149\n",
            "166 batch_normalization_149\n",
            "167 activation_149\n",
            "168 conv2d_150\n",
            "169 batch_normalization_150\n",
            "170 activation_150\n",
            "171 conv2d_146\n",
            "172 conv2d_151\n",
            "173 batch_normalization_146\n",
            "174 batch_normalization_151\n",
            "175 activation_146\n",
            "176 activation_151\n",
            "177 conv2d_147\n",
            "178 conv2d_152\n",
            "179 batch_normalization_147\n",
            "180 batch_normalization_152\n",
            "181 activation_147\n",
            "182 activation_152\n",
            "183 average_pooling2d_15\n",
            "184 conv2d_145\n",
            "185 conv2d_148\n",
            "186 conv2d_153\n",
            "187 conv2d_154\n",
            "188 batch_normalization_145\n",
            "189 batch_normalization_148\n",
            "190 batch_normalization_153\n",
            "191 batch_normalization_154\n",
            "192 activation_145\n",
            "193 activation_148\n",
            "194 activation_153\n",
            "195 activation_154\n",
            "196 mixed6\n",
            "197 conv2d_159\n",
            "198 batch_normalization_159\n",
            "199 activation_159\n",
            "200 conv2d_160\n",
            "201 batch_normalization_160\n",
            "202 activation_160\n",
            "203 conv2d_156\n",
            "204 conv2d_161\n",
            "205 batch_normalization_156\n",
            "206 batch_normalization_161\n",
            "207 activation_156\n",
            "208 activation_161\n",
            "209 conv2d_157\n",
            "210 conv2d_162\n",
            "211 batch_normalization_157\n",
            "212 batch_normalization_162\n",
            "213 activation_157\n",
            "214 activation_162\n",
            "215 average_pooling2d_16\n",
            "216 conv2d_155\n",
            "217 conv2d_158\n",
            "218 conv2d_163\n",
            "219 conv2d_164\n",
            "220 batch_normalization_155\n",
            "221 batch_normalization_158\n",
            "222 batch_normalization_163\n",
            "223 batch_normalization_164\n",
            "224 activation_155\n",
            "225 activation_158\n",
            "226 activation_163\n",
            "227 activation_164\n",
            "228 mixed7\n",
            "229 conv2d_167\n",
            "230 batch_normalization_167\n",
            "231 activation_167\n",
            "232 conv2d_168\n",
            "233 batch_normalization_168\n",
            "234 activation_168\n",
            "235 conv2d_165\n",
            "236 conv2d_169\n",
            "237 batch_normalization_165\n",
            "238 batch_normalization_169\n",
            "239 activation_165\n",
            "240 activation_169\n",
            "241 conv2d_166\n",
            "242 conv2d_170\n",
            "243 batch_normalization_166\n",
            "244 batch_normalization_170\n",
            "245 activation_166\n",
            "246 activation_170\n",
            "247 max_pooling2d_8\n",
            "248 mixed8\n",
            "249 conv2d_175\n",
            "250 batch_normalization_175\n",
            "251 activation_175\n",
            "252 conv2d_172\n",
            "253 conv2d_176\n",
            "254 batch_normalization_172\n",
            "255 batch_normalization_176\n",
            "256 activation_172\n",
            "257 activation_176\n",
            "258 conv2d_173\n",
            "259 conv2d_174\n",
            "260 conv2d_177\n",
            "261 conv2d_178\n",
            "262 average_pooling2d_17\n",
            "263 conv2d_171\n",
            "264 batch_normalization_173\n",
            "265 batch_normalization_174\n",
            "266 batch_normalization_177\n",
            "267 batch_normalization_178\n",
            "268 conv2d_179\n",
            "269 batch_normalization_171\n",
            "270 activation_173\n",
            "271 activation_174\n",
            "272 activation_177\n",
            "273 activation_178\n",
            "274 batch_normalization_179\n",
            "275 activation_171\n",
            "276 mixed9_0\n",
            "277 concatenate_3\n",
            "278 activation_179\n",
            "279 mixed9\n",
            "280 conv2d_184\n",
            "281 batch_normalization_184\n",
            "282 activation_184\n",
            "283 conv2d_181\n",
            "284 conv2d_185\n",
            "285 batch_normalization_181\n",
            "286 batch_normalization_185\n",
            "287 activation_181\n",
            "288 activation_185\n",
            "289 conv2d_182\n",
            "290 conv2d_183\n",
            "291 conv2d_186\n",
            "292 conv2d_187\n",
            "293 average_pooling2d_18\n",
            "294 conv2d_180\n",
            "295 batch_normalization_182\n",
            "296 batch_normalization_183\n",
            "297 batch_normalization_186\n",
            "298 batch_normalization_187\n",
            "299 conv2d_188\n",
            "300 batch_normalization_180\n",
            "301 activation_182\n",
            "302 activation_183\n",
            "303 activation_186\n",
            "304 activation_187\n",
            "305 batch_normalization_188\n",
            "306 activation_180\n",
            "307 mixed9_1\n",
            "308 concatenate_4\n",
            "309 activation_188\n",
            "310 mixed10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "peXB3nq9V6zB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "e41403fa-c05f-4fa7-a26c-85f1c2ed7b82"
      },
      "source": [
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(1024, activation = 'relu')(x)\n",
        "x = Dropout(0.5)(x)\n",
        "predictions = Dense(3, activation='softmax')(x)\n",
        "model = Model(inputs=base_model.input, outputs=predictions)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0717 16:30:23.522944 140452496836480 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EoEVuLslW6nr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "bf5c5371-0039-4074-bd28-1b8f0e203059"
      },
      "source": [
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "model.summary()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 299, 299, 3)  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 149, 149, 32) 864         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 149, 149, 32) 96          conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 149, 149, 32) 0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 147, 147, 32) 9216        activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 147, 147, 32) 96          conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 147, 147, 32) 0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 147, 147, 64) 18432       activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 147, 147, 64) 192         conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 147, 147, 64) 0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2D)  (None, 73, 73, 64)   0           activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 73, 73, 80)   5120        max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 73, 73, 80)   240         conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, 73, 73, 80)   0           batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 71, 71, 192)  138240      activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, 71, 71, 192)  576         conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_5 (Activation)       (None, 71, 71, 192)  0           batch_normalization_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2D)  (None, 35, 35, 192)  0           activation_5[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_9 (Conv2D)               (None, 35, 35, 64)   12288       max_pooling2d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_9 (BatchNor (None, 35, 35, 64)   192         conv2d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_9 (Activation)       (None, 35, 35, 64)   0           batch_normalization_9[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, 35, 35, 48)   9216        max_pooling2d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_10 (Conv2D)              (None, 35, 35, 96)   55296       activation_9[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_7 (BatchNor (None, 35, 35, 48)   144         conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_10 (BatchNo (None, 35, 35, 96)   288         conv2d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_7 (Activation)       (None, 35, 35, 48)   0           batch_normalization_7[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_10 (Activation)      (None, 35, 35, 96)   0           batch_normalization_10[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_1 (AveragePoo (None, 35, 35, 192)  0           max_pooling2d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, 35, 35, 64)   12288       max_pooling2d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_8 (Conv2D)               (None, 35, 35, 64)   76800       activation_7[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_11 (Conv2D)              (None, 35, 35, 96)   82944       activation_10[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_12 (Conv2D)              (None, 35, 35, 32)   6144        average_pooling2d_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_6 (BatchNor (None, 35, 35, 64)   192         conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_8 (BatchNor (None, 35, 35, 64)   192         conv2d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_11 (BatchNo (None, 35, 35, 96)   288         conv2d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_12 (BatchNo (None, 35, 35, 32)   96          conv2d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_6 (Activation)       (None, 35, 35, 64)   0           batch_normalization_6[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_8 (Activation)       (None, 35, 35, 64)   0           batch_normalization_8[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_11 (Activation)      (None, 35, 35, 96)   0           batch_normalization_11[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_12 (Activation)      (None, 35, 35, 32)   0           batch_normalization_12[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "mixed0 (Concatenate)            (None, 35, 35, 256)  0           activation_6[0][0]               \n",
            "                                                                 activation_8[0][0]               \n",
            "                                                                 activation_11[0][0]              \n",
            "                                                                 activation_12[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_16 (Conv2D)              (None, 35, 35, 64)   16384       mixed0[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_16 (BatchNo (None, 35, 35, 64)   192         conv2d_16[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_16 (Activation)      (None, 35, 35, 64)   0           batch_normalization_16[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_14 (Conv2D)              (None, 35, 35, 48)   12288       mixed0[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_17 (Conv2D)              (None, 35, 35, 96)   55296       activation_16[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_14 (BatchNo (None, 35, 35, 48)   144         conv2d_14[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_17 (BatchNo (None, 35, 35, 96)   288         conv2d_17[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_14 (Activation)      (None, 35, 35, 48)   0           batch_normalization_14[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_17 (Activation)      (None, 35, 35, 96)   0           batch_normalization_17[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_2 (AveragePoo (None, 35, 35, 256)  0           mixed0[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_13 (Conv2D)              (None, 35, 35, 64)   16384       mixed0[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_15 (Conv2D)              (None, 35, 35, 64)   76800       activation_14[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_18 (Conv2D)              (None, 35, 35, 96)   82944       activation_17[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_19 (Conv2D)              (None, 35, 35, 64)   16384       average_pooling2d_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_13 (BatchNo (None, 35, 35, 64)   192         conv2d_13[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_15 (BatchNo (None, 35, 35, 64)   192         conv2d_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_18 (BatchNo (None, 35, 35, 96)   288         conv2d_18[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_19 (BatchNo (None, 35, 35, 64)   192         conv2d_19[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_13 (Activation)      (None, 35, 35, 64)   0           batch_normalization_13[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_15 (Activation)      (None, 35, 35, 64)   0           batch_normalization_15[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_18 (Activation)      (None, 35, 35, 96)   0           batch_normalization_18[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_19 (Activation)      (None, 35, 35, 64)   0           batch_normalization_19[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "mixed1 (Concatenate)            (None, 35, 35, 288)  0           activation_13[0][0]              \n",
            "                                                                 activation_15[0][0]              \n",
            "                                                                 activation_18[0][0]              \n",
            "                                                                 activation_19[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_23 (Conv2D)              (None, 35, 35, 64)   18432       mixed1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_23 (BatchNo (None, 35, 35, 64)   192         conv2d_23[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_23 (Activation)      (None, 35, 35, 64)   0           batch_normalization_23[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_21 (Conv2D)              (None, 35, 35, 48)   13824       mixed1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_24 (Conv2D)              (None, 35, 35, 96)   55296       activation_23[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_21 (BatchNo (None, 35, 35, 48)   144         conv2d_21[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_24 (BatchNo (None, 35, 35, 96)   288         conv2d_24[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_21 (Activation)      (None, 35, 35, 48)   0           batch_normalization_21[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_24 (Activation)      (None, 35, 35, 96)   0           batch_normalization_24[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_3 (AveragePoo (None, 35, 35, 288)  0           mixed1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_20 (Conv2D)              (None, 35, 35, 64)   18432       mixed1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_22 (Conv2D)              (None, 35, 35, 64)   76800       activation_21[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_25 (Conv2D)              (None, 35, 35, 96)   82944       activation_24[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_26 (Conv2D)              (None, 35, 35, 64)   18432       average_pooling2d_3[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_20 (BatchNo (None, 35, 35, 64)   192         conv2d_20[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_22 (BatchNo (None, 35, 35, 64)   192         conv2d_22[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_25 (BatchNo (None, 35, 35, 96)   288         conv2d_25[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_26 (BatchNo (None, 35, 35, 64)   192         conv2d_26[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_20 (Activation)      (None, 35, 35, 64)   0           batch_normalization_20[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_22 (Activation)      (None, 35, 35, 64)   0           batch_normalization_22[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_25 (Activation)      (None, 35, 35, 96)   0           batch_normalization_25[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_26 (Activation)      (None, 35, 35, 64)   0           batch_normalization_26[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "mixed2 (Concatenate)            (None, 35, 35, 288)  0           activation_20[0][0]              \n",
            "                                                                 activation_22[0][0]              \n",
            "                                                                 activation_25[0][0]              \n",
            "                                                                 activation_26[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_28 (Conv2D)              (None, 35, 35, 64)   18432       mixed2[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_28 (BatchNo (None, 35, 35, 64)   192         conv2d_28[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_28 (Activation)      (None, 35, 35, 64)   0           batch_normalization_28[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_29 (Conv2D)              (None, 35, 35, 96)   55296       activation_28[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_29 (BatchNo (None, 35, 35, 96)   288         conv2d_29[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_29 (Activation)      (None, 35, 35, 96)   0           batch_normalization_29[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_27 (Conv2D)              (None, 17, 17, 384)  995328      mixed2[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_30 (Conv2D)              (None, 17, 17, 96)   82944       activation_29[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_27 (BatchNo (None, 17, 17, 384)  1152        conv2d_27[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_30 (BatchNo (None, 17, 17, 96)   288         conv2d_30[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_27 (Activation)      (None, 17, 17, 384)  0           batch_normalization_27[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_30 (Activation)      (None, 17, 17, 96)   0           batch_normalization_30[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2D)  (None, 17, 17, 288)  0           mixed2[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "mixed3 (Concatenate)            (None, 17, 17, 768)  0           activation_27[0][0]              \n",
            "                                                                 activation_30[0][0]              \n",
            "                                                                 max_pooling2d_3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_35 (Conv2D)              (None, 17, 17, 128)  98304       mixed3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_35 (BatchNo (None, 17, 17, 128)  384         conv2d_35[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_35 (Activation)      (None, 17, 17, 128)  0           batch_normalization_35[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_36 (Conv2D)              (None, 17, 17, 128)  114688      activation_35[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_36 (BatchNo (None, 17, 17, 128)  384         conv2d_36[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_36 (Activation)      (None, 17, 17, 128)  0           batch_normalization_36[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_32 (Conv2D)              (None, 17, 17, 128)  98304       mixed3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_37 (Conv2D)              (None, 17, 17, 128)  114688      activation_36[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_32 (BatchNo (None, 17, 17, 128)  384         conv2d_32[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_37 (BatchNo (None, 17, 17, 128)  384         conv2d_37[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_32 (Activation)      (None, 17, 17, 128)  0           batch_normalization_32[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_37 (Activation)      (None, 17, 17, 128)  0           batch_normalization_37[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_33 (Conv2D)              (None, 17, 17, 128)  114688      activation_32[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_38 (Conv2D)              (None, 17, 17, 128)  114688      activation_37[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_33 (BatchNo (None, 17, 17, 128)  384         conv2d_33[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_38 (BatchNo (None, 17, 17, 128)  384         conv2d_38[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_33 (Activation)      (None, 17, 17, 128)  0           batch_normalization_33[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_38 (Activation)      (None, 17, 17, 128)  0           batch_normalization_38[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_4 (AveragePoo (None, 17, 17, 768)  0           mixed3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_31 (Conv2D)              (None, 17, 17, 192)  147456      mixed3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_34 (Conv2D)              (None, 17, 17, 192)  172032      activation_33[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_39 (Conv2D)              (None, 17, 17, 192)  172032      activation_38[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_40 (Conv2D)              (None, 17, 17, 192)  147456      average_pooling2d_4[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_31 (BatchNo (None, 17, 17, 192)  576         conv2d_31[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_34 (BatchNo (None, 17, 17, 192)  576         conv2d_34[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_39 (BatchNo (None, 17, 17, 192)  576         conv2d_39[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_40 (BatchNo (None, 17, 17, 192)  576         conv2d_40[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_31 (Activation)      (None, 17, 17, 192)  0           batch_normalization_31[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_34 (Activation)      (None, 17, 17, 192)  0           batch_normalization_34[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_39 (Activation)      (None, 17, 17, 192)  0           batch_normalization_39[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_40 (Activation)      (None, 17, 17, 192)  0           batch_normalization_40[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "mixed4 (Concatenate)            (None, 17, 17, 768)  0           activation_31[0][0]              \n",
            "                                                                 activation_34[0][0]              \n",
            "                                                                 activation_39[0][0]              \n",
            "                                                                 activation_40[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_45 (Conv2D)              (None, 17, 17, 160)  122880      mixed4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_45 (BatchNo (None, 17, 17, 160)  480         conv2d_45[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_45 (Activation)      (None, 17, 17, 160)  0           batch_normalization_45[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_46 (Conv2D)              (None, 17, 17, 160)  179200      activation_45[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_46 (BatchNo (None, 17, 17, 160)  480         conv2d_46[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_46 (Activation)      (None, 17, 17, 160)  0           batch_normalization_46[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_42 (Conv2D)              (None, 17, 17, 160)  122880      mixed4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_47 (Conv2D)              (None, 17, 17, 160)  179200      activation_46[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_42 (BatchNo (None, 17, 17, 160)  480         conv2d_42[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_47 (BatchNo (None, 17, 17, 160)  480         conv2d_47[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_42 (Activation)      (None, 17, 17, 160)  0           batch_normalization_42[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_47 (Activation)      (None, 17, 17, 160)  0           batch_normalization_47[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_43 (Conv2D)              (None, 17, 17, 160)  179200      activation_42[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_48 (Conv2D)              (None, 17, 17, 160)  179200      activation_47[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_43 (BatchNo (None, 17, 17, 160)  480         conv2d_43[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_48 (BatchNo (None, 17, 17, 160)  480         conv2d_48[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_43 (Activation)      (None, 17, 17, 160)  0           batch_normalization_43[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_48 (Activation)      (None, 17, 17, 160)  0           batch_normalization_48[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_5 (AveragePoo (None, 17, 17, 768)  0           mixed4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_41 (Conv2D)              (None, 17, 17, 192)  147456      mixed4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_44 (Conv2D)              (None, 17, 17, 192)  215040      activation_43[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_49 (Conv2D)              (None, 17, 17, 192)  215040      activation_48[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_50 (Conv2D)              (None, 17, 17, 192)  147456      average_pooling2d_5[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_41 (BatchNo (None, 17, 17, 192)  576         conv2d_41[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_44 (BatchNo (None, 17, 17, 192)  576         conv2d_44[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_49 (BatchNo (None, 17, 17, 192)  576         conv2d_49[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_50 (BatchNo (None, 17, 17, 192)  576         conv2d_50[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_41 (Activation)      (None, 17, 17, 192)  0           batch_normalization_41[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_44 (Activation)      (None, 17, 17, 192)  0           batch_normalization_44[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_49 (Activation)      (None, 17, 17, 192)  0           batch_normalization_49[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_50 (Activation)      (None, 17, 17, 192)  0           batch_normalization_50[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "mixed5 (Concatenate)            (None, 17, 17, 768)  0           activation_41[0][0]              \n",
            "                                                                 activation_44[0][0]              \n",
            "                                                                 activation_49[0][0]              \n",
            "                                                                 activation_50[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_55 (Conv2D)              (None, 17, 17, 160)  122880      mixed5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_55 (BatchNo (None, 17, 17, 160)  480         conv2d_55[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_55 (Activation)      (None, 17, 17, 160)  0           batch_normalization_55[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_56 (Conv2D)              (None, 17, 17, 160)  179200      activation_55[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_56 (BatchNo (None, 17, 17, 160)  480         conv2d_56[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_56 (Activation)      (None, 17, 17, 160)  0           batch_normalization_56[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_52 (Conv2D)              (None, 17, 17, 160)  122880      mixed5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_57 (Conv2D)              (None, 17, 17, 160)  179200      activation_56[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_52 (BatchNo (None, 17, 17, 160)  480         conv2d_52[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_57 (BatchNo (None, 17, 17, 160)  480         conv2d_57[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_52 (Activation)      (None, 17, 17, 160)  0           batch_normalization_52[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_57 (Activation)      (None, 17, 17, 160)  0           batch_normalization_57[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_53 (Conv2D)              (None, 17, 17, 160)  179200      activation_52[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_58 (Conv2D)              (None, 17, 17, 160)  179200      activation_57[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_53 (BatchNo (None, 17, 17, 160)  480         conv2d_53[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_58 (BatchNo (None, 17, 17, 160)  480         conv2d_58[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_53 (Activation)      (None, 17, 17, 160)  0           batch_normalization_53[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_58 (Activation)      (None, 17, 17, 160)  0           batch_normalization_58[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_6 (AveragePoo (None, 17, 17, 768)  0           mixed5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_51 (Conv2D)              (None, 17, 17, 192)  147456      mixed5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_54 (Conv2D)              (None, 17, 17, 192)  215040      activation_53[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_59 (Conv2D)              (None, 17, 17, 192)  215040      activation_58[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_60 (Conv2D)              (None, 17, 17, 192)  147456      average_pooling2d_6[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_51 (BatchNo (None, 17, 17, 192)  576         conv2d_51[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_54 (BatchNo (None, 17, 17, 192)  576         conv2d_54[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_59 (BatchNo (None, 17, 17, 192)  576         conv2d_59[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_60 (BatchNo (None, 17, 17, 192)  576         conv2d_60[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_51 (Activation)      (None, 17, 17, 192)  0           batch_normalization_51[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_54 (Activation)      (None, 17, 17, 192)  0           batch_normalization_54[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_59 (Activation)      (None, 17, 17, 192)  0           batch_normalization_59[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_60 (Activation)      (None, 17, 17, 192)  0           batch_normalization_60[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "mixed6 (Concatenate)            (None, 17, 17, 768)  0           activation_51[0][0]              \n",
            "                                                                 activation_54[0][0]              \n",
            "                                                                 activation_59[0][0]              \n",
            "                                                                 activation_60[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_65 (Conv2D)              (None, 17, 17, 192)  147456      mixed6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_65 (BatchNo (None, 17, 17, 192)  576         conv2d_65[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_65 (Activation)      (None, 17, 17, 192)  0           batch_normalization_65[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_66 (Conv2D)              (None, 17, 17, 192)  258048      activation_65[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_66 (BatchNo (None, 17, 17, 192)  576         conv2d_66[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_66 (Activation)      (None, 17, 17, 192)  0           batch_normalization_66[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_62 (Conv2D)              (None, 17, 17, 192)  147456      mixed6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_67 (Conv2D)              (None, 17, 17, 192)  258048      activation_66[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_62 (BatchNo (None, 17, 17, 192)  576         conv2d_62[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_67 (BatchNo (None, 17, 17, 192)  576         conv2d_67[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_62 (Activation)      (None, 17, 17, 192)  0           batch_normalization_62[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_67 (Activation)      (None, 17, 17, 192)  0           batch_normalization_67[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_63 (Conv2D)              (None, 17, 17, 192)  258048      activation_62[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_68 (Conv2D)              (None, 17, 17, 192)  258048      activation_67[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_63 (BatchNo (None, 17, 17, 192)  576         conv2d_63[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_68 (BatchNo (None, 17, 17, 192)  576         conv2d_68[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_63 (Activation)      (None, 17, 17, 192)  0           batch_normalization_63[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_68 (Activation)      (None, 17, 17, 192)  0           batch_normalization_68[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_7 (AveragePoo (None, 17, 17, 768)  0           mixed6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_61 (Conv2D)              (None, 17, 17, 192)  147456      mixed6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_64 (Conv2D)              (None, 17, 17, 192)  258048      activation_63[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_69 (Conv2D)              (None, 17, 17, 192)  258048      activation_68[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_70 (Conv2D)              (None, 17, 17, 192)  147456      average_pooling2d_7[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_61 (BatchNo (None, 17, 17, 192)  576         conv2d_61[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_64 (BatchNo (None, 17, 17, 192)  576         conv2d_64[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_69 (BatchNo (None, 17, 17, 192)  576         conv2d_69[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_70 (BatchNo (None, 17, 17, 192)  576         conv2d_70[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_61 (Activation)      (None, 17, 17, 192)  0           batch_normalization_61[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_64 (Activation)      (None, 17, 17, 192)  0           batch_normalization_64[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_69 (Activation)      (None, 17, 17, 192)  0           batch_normalization_69[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_70 (Activation)      (None, 17, 17, 192)  0           batch_normalization_70[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "mixed7 (Concatenate)            (None, 17, 17, 768)  0           activation_61[0][0]              \n",
            "                                                                 activation_64[0][0]              \n",
            "                                                                 activation_69[0][0]              \n",
            "                                                                 activation_70[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_73 (Conv2D)              (None, 17, 17, 192)  147456      mixed7[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_73 (BatchNo (None, 17, 17, 192)  576         conv2d_73[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_73 (Activation)      (None, 17, 17, 192)  0           batch_normalization_73[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_74 (Conv2D)              (None, 17, 17, 192)  258048      activation_73[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_74 (BatchNo (None, 17, 17, 192)  576         conv2d_74[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_74 (Activation)      (None, 17, 17, 192)  0           batch_normalization_74[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_71 (Conv2D)              (None, 17, 17, 192)  147456      mixed7[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_75 (Conv2D)              (None, 17, 17, 192)  258048      activation_74[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_71 (BatchNo (None, 17, 17, 192)  576         conv2d_71[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_75 (BatchNo (None, 17, 17, 192)  576         conv2d_75[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_71 (Activation)      (None, 17, 17, 192)  0           batch_normalization_71[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_75 (Activation)      (None, 17, 17, 192)  0           batch_normalization_75[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_72 (Conv2D)              (None, 8, 8, 320)    552960      activation_71[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_76 (Conv2D)              (None, 8, 8, 192)    331776      activation_75[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_72 (BatchNo (None, 8, 8, 320)    960         conv2d_72[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_76 (BatchNo (None, 8, 8, 192)    576         conv2d_76[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_72 (Activation)      (None, 8, 8, 320)    0           batch_normalization_72[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_76 (Activation)      (None, 8, 8, 192)    0           batch_normalization_76[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2D)  (None, 8, 8, 768)    0           mixed7[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "mixed8 (Concatenate)            (None, 8, 8, 1280)   0           activation_72[0][0]              \n",
            "                                                                 activation_76[0][0]              \n",
            "                                                                 max_pooling2d_4[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_81 (Conv2D)              (None, 8, 8, 448)    573440      mixed8[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_81 (BatchNo (None, 8, 8, 448)    1344        conv2d_81[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_81 (Activation)      (None, 8, 8, 448)    0           batch_normalization_81[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_78 (Conv2D)              (None, 8, 8, 384)    491520      mixed8[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_82 (Conv2D)              (None, 8, 8, 384)    1548288     activation_81[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_78 (BatchNo (None, 8, 8, 384)    1152        conv2d_78[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_82 (BatchNo (None, 8, 8, 384)    1152        conv2d_82[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_78 (Activation)      (None, 8, 8, 384)    0           batch_normalization_78[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_82 (Activation)      (None, 8, 8, 384)    0           batch_normalization_82[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_79 (Conv2D)              (None, 8, 8, 384)    442368      activation_78[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_80 (Conv2D)              (None, 8, 8, 384)    442368      activation_78[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_83 (Conv2D)              (None, 8, 8, 384)    442368      activation_82[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_84 (Conv2D)              (None, 8, 8, 384)    442368      activation_82[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_8 (AveragePoo (None, 8, 8, 1280)   0           mixed8[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_77 (Conv2D)              (None, 8, 8, 320)    409600      mixed8[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_79 (BatchNo (None, 8, 8, 384)    1152        conv2d_79[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_80 (BatchNo (None, 8, 8, 384)    1152        conv2d_80[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_83 (BatchNo (None, 8, 8, 384)    1152        conv2d_83[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_84 (BatchNo (None, 8, 8, 384)    1152        conv2d_84[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_85 (Conv2D)              (None, 8, 8, 192)    245760      average_pooling2d_8[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_77 (BatchNo (None, 8, 8, 320)    960         conv2d_77[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_79 (Activation)      (None, 8, 8, 384)    0           batch_normalization_79[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_80 (Activation)      (None, 8, 8, 384)    0           batch_normalization_80[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_83 (Activation)      (None, 8, 8, 384)    0           batch_normalization_83[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_84 (Activation)      (None, 8, 8, 384)    0           batch_normalization_84[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_85 (BatchNo (None, 8, 8, 192)    576         conv2d_85[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_77 (Activation)      (None, 8, 8, 320)    0           batch_normalization_77[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "mixed9_0 (Concatenate)          (None, 8, 8, 768)    0           activation_79[0][0]              \n",
            "                                                                 activation_80[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 8, 8, 768)    0           activation_83[0][0]              \n",
            "                                                                 activation_84[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_85 (Activation)      (None, 8, 8, 192)    0           batch_normalization_85[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "mixed9 (Concatenate)            (None, 8, 8, 2048)   0           activation_77[0][0]              \n",
            "                                                                 mixed9_0[0][0]                   \n",
            "                                                                 concatenate_1[0][0]              \n",
            "                                                                 activation_85[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_90 (Conv2D)              (None, 8, 8, 448)    917504      mixed9[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_90 (BatchNo (None, 8, 8, 448)    1344        conv2d_90[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_90 (Activation)      (None, 8, 8, 448)    0           batch_normalization_90[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_87 (Conv2D)              (None, 8, 8, 384)    786432      mixed9[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_91 (Conv2D)              (None, 8, 8, 384)    1548288     activation_90[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_87 (BatchNo (None, 8, 8, 384)    1152        conv2d_87[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_91 (BatchNo (None, 8, 8, 384)    1152        conv2d_91[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_87 (Activation)      (None, 8, 8, 384)    0           batch_normalization_87[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_91 (Activation)      (None, 8, 8, 384)    0           batch_normalization_91[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_88 (Conv2D)              (None, 8, 8, 384)    442368      activation_87[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_89 (Conv2D)              (None, 8, 8, 384)    442368      activation_87[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_92 (Conv2D)              (None, 8, 8, 384)    442368      activation_91[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_93 (Conv2D)              (None, 8, 8, 384)    442368      activation_91[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_9 (AveragePoo (None, 8, 8, 2048)   0           mixed9[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_86 (Conv2D)              (None, 8, 8, 320)    655360      mixed9[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_88 (BatchNo (None, 8, 8, 384)    1152        conv2d_88[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_89 (BatchNo (None, 8, 8, 384)    1152        conv2d_89[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_92 (BatchNo (None, 8, 8, 384)    1152        conv2d_92[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_93 (BatchNo (None, 8, 8, 384)    1152        conv2d_93[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_94 (Conv2D)              (None, 8, 8, 192)    393216      average_pooling2d_9[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_86 (BatchNo (None, 8, 8, 320)    960         conv2d_86[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_88 (Activation)      (None, 8, 8, 384)    0           batch_normalization_88[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_89 (Activation)      (None, 8, 8, 384)    0           batch_normalization_89[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_92 (Activation)      (None, 8, 8, 384)    0           batch_normalization_92[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_93 (Activation)      (None, 8, 8, 384)    0           batch_normalization_93[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_94 (BatchNo (None, 8, 8, 192)    576         conv2d_94[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_86 (Activation)      (None, 8, 8, 320)    0           batch_normalization_86[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "mixed9_1 (Concatenate)          (None, 8, 8, 768)    0           activation_88[0][0]              \n",
            "                                                                 activation_89[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_2 (Concatenate)     (None, 8, 8, 768)    0           activation_92[0][0]              \n",
            "                                                                 activation_93[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_94 (Activation)      (None, 8, 8, 192)    0           batch_normalization_94[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "mixed10 (Concatenate)           (None, 8, 8, 2048)   0           activation_86[0][0]              \n",
            "                                                                 mixed9_1[0][0]                   \n",
            "                                                                 concatenate_2[0][0]              \n",
            "                                                                 activation_94[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling2d_1 (Glo (None, 2048)         0           mixed10[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 1024)         2098176     global_average_pooling2d_1[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 1024)         0           dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 3)            3075        dropout_1[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 23,904,035\n",
            "Trainable params: 2,101,251\n",
            "Non-trainable params: 21,802,784\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6v0hyOA1XRCe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "73aa9643-5c36-4058-dce6-398860cb989f"
      },
      "source": [
        "optimizer = Adam(lr=1e-6)\n",
        "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0717 16:33:48.526302 140452496836480 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4lqfwwzO_TIG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wGv9CrlTXYF-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Baixar as imagens do exame de citologia cervical\n",
        "!wget -cq https://citologia-cervical.s3-sa-east-1.amazonaws.com/citologia.zip\n",
        "!unzip -qq citologia.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OA9tcwNcXglg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_datagen=ImageDataGenerator(preprocessing_function = preprocess_input) # incluÃ­do nas dependÃªncias\n",
        "\n",
        "validation_datagen = ImageDataGenerator(preprocessing_function = preprocess_input)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5P-LzFbXtnJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "b00e09df-d555-4fb8-8336-fab6da30e636"
      },
      "source": [
        "training_set = train_datagen.flow_from_directory('train',\n",
        "                                                 target_size = (img_width, img_height),\n",
        "                                                 color_mode='rgb',\n",
        "                                                 batch_size = 32,\n",
        "                                                 class_mode = 'categorical',\n",
        "                                                 shuffle=True)\n",
        "\n",
        "validation_set = validation_datagen.flow_from_directory('valid',\n",
        "                                                        target_size = (img_width, img_height),\n",
        "                                                        color_mode='rgb',\n",
        "                                                        batch_size = 32,\n",
        "                                                        class_mode = 'categorical',\n",
        "                                                        shuffle=True)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 423 images belonging to 3 classes.\n",
            "Found 105 images belonging to 3 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PItWeVfHXws5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        },
        "outputId": "bbbfdc14-b5d2-4085-d14a-19c67864e12e"
      },
      "source": [
        "history = model.fit_generator(training_set,\n",
        "                    steps_per_epoch=528/32,\n",
        "                    epochs = 20,\n",
        "                    validation_data = validation_set,\n",
        "                    validation_steps = 105/32)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "17/16 [==============================] - 17s 977ms/step - loss: 3.5693 - acc: 0.4014 - val_loss: 7.3988 - val_acc: 0.4286\n",
            "Epoch 2/20\n",
            "17/16 [==============================] - 8s 452ms/step - loss: 1.3984 - acc: 0.6109 - val_loss: 3.3683 - val_acc: 0.5524\n",
            "Epoch 3/20\n",
            "17/16 [==============================] - 8s 452ms/step - loss: 0.7120 - acc: 0.7498 - val_loss: 8.7976 - val_acc: 0.4190\n",
            "Epoch 4/20\n",
            "17/16 [==============================] - 8s 453ms/step - loss: 0.2924 - acc: 0.8943 - val_loss: 9.7229 - val_acc: 0.3905\n",
            "Epoch 5/20\n",
            "17/16 [==============================] - 8s 443ms/step - loss: 0.2125 - acc: 0.9296 - val_loss: 8.2746 - val_acc: 0.4381\n",
            "Epoch 6/20\n",
            "17/16 [==============================] - 8s 451ms/step - loss: 0.1975 - acc: 0.9311 - val_loss: 9.7465 - val_acc: 0.3905\n",
            "Epoch 7/20\n",
            "17/16 [==============================] - 8s 452ms/step - loss: 0.3006 - acc: 0.9360 - val_loss: 6.6510 - val_acc: 0.4952\n",
            "Epoch 8/20\n",
            "17/16 [==============================] - 8s 451ms/step - loss: 0.1584 - acc: 0.9558 - val_loss: 5.9584 - val_acc: 0.5143\n",
            "Epoch 9/20\n",
            "17/16 [==============================] - 8s 454ms/step - loss: 0.1058 - acc: 0.9680 - val_loss: 6.7248 - val_acc: 0.4667\n",
            "Epoch 10/20\n",
            "17/16 [==============================] - 7s 438ms/step - loss: 0.0832 - acc: 0.9760 - val_loss: 8.2611 - val_acc: 0.4190\n",
            "Epoch 11/20\n",
            "17/16 [==============================] - 8s 451ms/step - loss: 0.0747 - acc: 0.9797 - val_loss: 7.0807 - val_acc: 0.4667\n",
            "Epoch 12/20\n",
            "17/16 [==============================] - 8s 452ms/step - loss: 0.1187 - acc: 0.9625 - val_loss: 5.6890 - val_acc: 0.5143\n",
            "Epoch 13/20\n",
            "17/16 [==============================] - 8s 452ms/step - loss: 0.0253 - acc: 0.9908 - val_loss: 4.9642 - val_acc: 0.4762\n",
            "Epoch 14/20\n",
            "17/16 [==============================] - 8s 454ms/step - loss: 0.1549 - acc: 0.9459 - val_loss: 7.2105 - val_acc: 0.4667\n",
            "Epoch 15/20\n",
            "17/16 [==============================] - 7s 438ms/step - loss: 0.0946 - acc: 0.9772 - val_loss: 8.1753 - val_acc: 0.4476\n",
            "Epoch 16/20\n",
            "17/16 [==============================] - 8s 452ms/step - loss: 0.0413 - acc: 0.9871 - val_loss: 10.2001 - val_acc: 0.3619\n",
            "Epoch 17/20\n",
            "17/16 [==============================] - 8s 454ms/step - loss: 0.1198 - acc: 0.9779 - val_loss: 5.8443 - val_acc: 0.5524\n",
            "Epoch 18/20\n",
            "17/16 [==============================] - 8s 454ms/step - loss: 0.0309 - acc: 0.9871 - val_loss: 6.7522 - val_acc: 0.5143\n",
            "Epoch 19/20\n",
            "17/16 [==============================] - 7s 440ms/step - loss: 0.0883 - acc: 0.9778 - val_loss: 3.8072 - val_acc: 0.5619\n",
            "Epoch 20/20\n",
            "17/16 [==============================] - 8s 455ms/step - loss: 0.1630 - acc: 0.9791 - val_loss: 5.9385 - val_acc: 0.5429\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TkjWX09ay8eZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for layer in model.layers[:249]:\n",
        "   layer.trainable = False\n",
        "for layer in model.layers[249:]:\n",
        "   layer.trainable = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jpRqljYqzDq5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.optimizers import SGD\n",
        "model.compile(optimizer=SGD(lr=0.000001, momentum=0.9), loss='categorical_crossentropy', metrics=['accuracy'])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7BuZab7lzLSR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "61c71bdc-7b03-4f6a-a569-2184638b4f0f"
      },
      "source": [
        "from google.colab import drive\n",
        "from keras.callbacks import *\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "filepath=\"/content/drive/My Drive/Colab Notebooks/log/inceptionv3:{epoch:03d}-val_acc:{val_acc:.3f}.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
        "callbacks_list = [checkpoint]"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-aTaRIsdzdDR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8fa108a5-c0a1-48f2-f27d-30ebbd1a5eb3"
      },
      "source": [
        "# Fine-tune\n",
        "history = model.fit_generator(training_set,\n",
        "                    steps_per_epoch=528/32,\n",
        "                    epochs = 100,\n",
        "                    validation_data = validation_set,\n",
        "                    validation_steps = 105/32,\n",
        "                    callbacks=callbacks_list)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "17/16 [==============================] - 16s 962ms/step - loss: 0.0852 - acc: 0.9760 - val_loss: 5.8975 - val_acc: 0.5429\n",
            "\n",
            "Epoch 00001: val_acc improved from 0.50476 to 0.54286, saving model to /content/drive/My Drive/Colab Notebooks/log/inceptionv3:001-val_acc:0.543.hdf5\n",
            "Epoch 2/100\n",
            "17/16 [==============================] - 7s 440ms/step - loss: 0.0749 - acc: 0.9760 - val_loss: 4.5679 - val_acc: 0.5905\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.54286 to 0.59048, saving model to /content/drive/My Drive/Colab Notebooks/log/inceptionv3:002-val_acc:0.590.hdf5\n",
            "Epoch 3/100\n",
            "17/16 [==============================] - 7s 440ms/step - loss: 0.0566 - acc: 0.9816 - val_loss: 4.2595 - val_acc: 0.5714\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.59048\n",
            "Epoch 4/100\n",
            "17/16 [==============================] - 8s 447ms/step - loss: 0.1645 - acc: 0.9765 - val_loss: 3.3156 - val_acc: 0.6000\n",
            "\n",
            "Epoch 00004: val_acc improved from 0.59048 to 0.60000, saving model to /content/drive/My Drive/Colab Notebooks/log/inceptionv3:004-val_acc:0.600.hdf5\n",
            "Epoch 5/100\n",
            "17/16 [==============================] - 7s 424ms/step - loss: 0.0823 - acc: 0.9747 - val_loss: 4.0590 - val_acc: 0.5143\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.60000\n",
            "Epoch 6/100\n",
            "17/16 [==============================] - 8s 444ms/step - loss: 0.0372 - acc: 0.9853 - val_loss: 3.5322 - val_acc: 0.6095\n",
            "\n",
            "Epoch 00006: val_acc improved from 0.60000 to 0.60952, saving model to /content/drive/My Drive/Colab Notebooks/log/inceptionv3:006-val_acc:0.610.hdf5\n",
            "Epoch 7/100\n",
            "17/16 [==============================] - 7s 439ms/step - loss: 0.0549 - acc: 0.9889 - val_loss: 2.8670 - val_acc: 0.6000\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.60952\n",
            "Epoch 8/100\n",
            "17/16 [==============================] - 8s 444ms/step - loss: 0.0483 - acc: 0.9754 - val_loss: 3.3730 - val_acc: 0.6000\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.60952\n",
            "Epoch 9/100\n",
            "17/16 [==============================] - 8s 443ms/step - loss: 0.0920 - acc: 0.9809 - val_loss: 2.3170 - val_acc: 0.6952\n",
            "\n",
            "Epoch 00009: val_acc improved from 0.60952 to 0.69524, saving model to /content/drive/My Drive/Colab Notebooks/log/inceptionv3:009-val_acc:0.695.hdf5\n",
            "Epoch 10/100\n",
            "17/16 [==============================] - 7s 430ms/step - loss: 0.0484 - acc: 0.9852 - val_loss: 2.8455 - val_acc: 0.6000\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.69524\n",
            "Epoch 11/100\n",
            "17/16 [==============================] - 8s 449ms/step - loss: 0.0928 - acc: 0.9809 - val_loss: 3.6346 - val_acc: 0.5810\n",
            "\n",
            "Epoch 00011: val_acc did not improve from 0.69524\n",
            "Epoch 12/100\n",
            "17/16 [==============================] - 8s 442ms/step - loss: 0.0577 - acc: 0.9889 - val_loss: 2.5052 - val_acc: 0.6190\n",
            "\n",
            "Epoch 00012: val_acc did not improve from 0.69524\n",
            "Epoch 13/100\n",
            "17/16 [==============================] - 8s 445ms/step - loss: 0.1026 - acc: 0.9698 - val_loss: 3.2909 - val_acc: 0.5333\n",
            "\n",
            "Epoch 00013: val_acc did not improve from 0.69524\n",
            "Epoch 14/100\n",
            "17/16 [==============================] - 7s 433ms/step - loss: 0.0406 - acc: 0.9908 - val_loss: 2.6435 - val_acc: 0.5905\n",
            "\n",
            "Epoch 00014: val_acc did not improve from 0.69524\n",
            "Epoch 15/100\n",
            "17/16 [==============================] - 8s 442ms/step - loss: 0.1625 - acc: 0.9568 - val_loss: 2.9355 - val_acc: 0.6000\n",
            "\n",
            "Epoch 00015: val_acc did not improve from 0.69524\n",
            "Epoch 16/100\n",
            "17/16 [==============================] - 8s 444ms/step - loss: 0.0631 - acc: 0.9791 - val_loss: 2.5236 - val_acc: 0.5905\n",
            "\n",
            "Epoch 00016: val_acc did not improve from 0.69524\n",
            "Epoch 17/100\n",
            "17/16 [==============================] - 8s 442ms/step - loss: 0.0487 - acc: 0.9853 - val_loss: 3.1885 - val_acc: 0.5429\n",
            "\n",
            "Epoch 00017: val_acc did not improve from 0.69524\n",
            "Epoch 18/100\n",
            "17/16 [==============================] - 8s 446ms/step - loss: 0.0540 - acc: 0.9834 - val_loss: 3.1627 - val_acc: 0.5429\n",
            "\n",
            "Epoch 00018: val_acc did not improve from 0.69524\n",
            "Epoch 19/100\n",
            "17/16 [==============================] - 7s 429ms/step - loss: 0.1010 - acc: 0.9728 - val_loss: 2.2477 - val_acc: 0.6095\n",
            "\n",
            "Epoch 00019: val_acc did not improve from 0.69524\n",
            "Epoch 20/100\n",
            "17/16 [==============================] - 8s 442ms/step - loss: 0.0597 - acc: 0.9853 - val_loss: 2.9714 - val_acc: 0.5619\n",
            "\n",
            "Epoch 00020: val_acc did not improve from 0.69524\n",
            "Epoch 21/100\n",
            "17/16 [==============================] - 8s 444ms/step - loss: 0.0558 - acc: 0.9834 - val_loss: 3.4134 - val_acc: 0.5048\n",
            "\n",
            "Epoch 00021: val_acc did not improve from 0.69524\n",
            "Epoch 22/100\n",
            "17/16 [==============================] - 8s 444ms/step - loss: 0.0751 - acc: 0.9834 - val_loss: 2.5531 - val_acc: 0.6095\n",
            "\n",
            "Epoch 00022: val_acc did not improve from 0.69524\n",
            "Epoch 23/100\n",
            "17/16 [==============================] - 8s 443ms/step - loss: 0.0375 - acc: 0.9871 - val_loss: 2.5525 - val_acc: 0.5429\n",
            "\n",
            "Epoch 00023: val_acc did not improve from 0.69524\n",
            "Epoch 24/100\n",
            "17/16 [==============================] - 7s 431ms/step - loss: 0.0961 - acc: 0.9654 - val_loss: 2.6457 - val_acc: 0.5619\n",
            "\n",
            "Epoch 00024: val_acc did not improve from 0.69524\n",
            "Epoch 25/100\n",
            "17/16 [==============================] - 8s 443ms/step - loss: 0.0380 - acc: 0.9908 - val_loss: 2.9591 - val_acc: 0.5810\n",
            "\n",
            "Epoch 00025: val_acc did not improve from 0.69524\n",
            "Epoch 26/100\n",
            "17/16 [==============================] - 8s 444ms/step - loss: 0.0646 - acc: 0.9673 - val_loss: 3.0399 - val_acc: 0.5619\n",
            "\n",
            "Epoch 00026: val_acc did not improve from 0.69524\n",
            "Epoch 27/100\n",
            "17/16 [==============================] - 8s 445ms/step - loss: 0.0497 - acc: 0.9908 - val_loss: 2.3770 - val_acc: 0.5238\n",
            "\n",
            "Epoch 00027: val_acc did not improve from 0.69524\n",
            "Epoch 28/100\n",
            "17/16 [==============================] - 7s 429ms/step - loss: 0.0880 - acc: 0.9834 - val_loss: 2.9209 - val_acc: 0.5333\n",
            "\n",
            "Epoch 00028: val_acc did not improve from 0.69524\n",
            "Epoch 29/100\n",
            "17/16 [==============================] - 8s 444ms/step - loss: 0.0612 - acc: 0.9834 - val_loss: 2.7863 - val_acc: 0.6190\n",
            "\n",
            "Epoch 00029: val_acc did not improve from 0.69524\n",
            "Epoch 30/100\n",
            "17/16 [==============================] - 8s 444ms/step - loss: 0.2002 - acc: 0.9747 - val_loss: 3.0868 - val_acc: 0.5143\n",
            "\n",
            "Epoch 00030: val_acc did not improve from 0.69524\n",
            "Epoch 31/100\n",
            "17/16 [==============================] - 8s 446ms/step - loss: 0.1754 - acc: 0.9611 - val_loss: 3.1317 - val_acc: 0.4857\n",
            "\n",
            "Epoch 00031: val_acc did not improve from 0.69524\n",
            "Epoch 32/100\n",
            "17/16 [==============================] - 8s 442ms/step - loss: 0.0362 - acc: 0.9908 - val_loss: 2.6302 - val_acc: 0.5905\n",
            "\n",
            "Epoch 00032: val_acc did not improve from 0.69524\n",
            "Epoch 33/100\n",
            "17/16 [==============================] - 7s 428ms/step - loss: 0.1452 - acc: 0.9710 - val_loss: 2.7580 - val_acc: 0.5524\n",
            "\n",
            "Epoch 00033: val_acc did not improve from 0.69524\n",
            "Epoch 34/100\n",
            "17/16 [==============================] - 8s 446ms/step - loss: 0.1032 - acc: 0.9692 - val_loss: 2.6071 - val_acc: 0.6000\n",
            "\n",
            "Epoch 00034: val_acc did not improve from 0.69524\n",
            "Epoch 35/100\n",
            "17/16 [==============================] - 8s 444ms/step - loss: 0.0555 - acc: 0.9834 - val_loss: 2.4891 - val_acc: 0.5810\n",
            "\n",
            "Epoch 00035: val_acc did not improve from 0.69524\n",
            "Epoch 36/100\n",
            "17/16 [==============================] - 8s 443ms/step - loss: 0.0310 - acc: 0.9908 - val_loss: 3.4694 - val_acc: 0.4762\n",
            "\n",
            "Epoch 00036: val_acc did not improve from 0.69524\n",
            "Epoch 37/100\n",
            "17/16 [==============================] - 8s 441ms/step - loss: 0.1571 - acc: 0.9519 - val_loss: 2.6827 - val_acc: 0.5524\n",
            "\n",
            "Epoch 00037: val_acc did not improve from 0.69524\n",
            "Epoch 38/100\n",
            "17/16 [==============================] - 7s 429ms/step - loss: 0.1523 - acc: 0.9673 - val_loss: 2.4763 - val_acc: 0.6000\n",
            "\n",
            "Epoch 00038: val_acc did not improve from 0.69524\n",
            "Epoch 39/100\n",
            "17/16 [==============================] - 8s 445ms/step - loss: 0.0404 - acc: 0.9889 - val_loss: 3.0250 - val_acc: 0.5524\n",
            "\n",
            "Epoch 00039: val_acc did not improve from 0.69524\n",
            "Epoch 40/100\n",
            "17/16 [==============================] - 8s 444ms/step - loss: 0.0975 - acc: 0.9729 - val_loss: 2.3177 - val_acc: 0.6095\n",
            "\n",
            "Epoch 00040: val_acc did not improve from 0.69524\n",
            "Epoch 41/100\n",
            "17/16 [==============================] - 8s 446ms/step - loss: 0.1220 - acc: 0.9827 - val_loss: 2.9910 - val_acc: 0.5429\n",
            "\n",
            "Epoch 00041: val_acc did not improve from 0.69524\n",
            "Epoch 42/100\n",
            "17/16 [==============================] - 7s 428ms/step - loss: 0.0366 - acc: 0.9864 - val_loss: 3.0824 - val_acc: 0.4952\n",
            "\n",
            "Epoch 00042: val_acc did not improve from 0.69524\n",
            "Epoch 43/100\n",
            "17/16 [==============================] - 8s 442ms/step - loss: 0.0843 - acc: 0.9710 - val_loss: 2.7819 - val_acc: 0.4952\n",
            "\n",
            "Epoch 00043: val_acc did not improve from 0.69524\n",
            "Epoch 44/100\n",
            "17/16 [==============================] - 8s 445ms/step - loss: 0.0427 - acc: 0.9834 - val_loss: 3.1229 - val_acc: 0.5619\n",
            "\n",
            "Epoch 00044: val_acc did not improve from 0.69524\n",
            "Epoch 45/100\n",
            "17/16 [==============================] - 8s 444ms/step - loss: 0.0862 - acc: 0.9765 - val_loss: 2.5531 - val_acc: 0.5524\n",
            "\n",
            "Epoch 00045: val_acc did not improve from 0.69524\n",
            "Epoch 46/100\n",
            "17/16 [==============================] - 8s 441ms/step - loss: 0.0406 - acc: 0.9871 - val_loss: 2.8444 - val_acc: 0.5714\n",
            "\n",
            "Epoch 00046: val_acc did not improve from 0.69524\n",
            "Epoch 47/100\n",
            "17/16 [==============================] - 7s 429ms/step - loss: 0.0925 - acc: 0.9710 - val_loss: 2.8051 - val_acc: 0.5619\n",
            "\n",
            "Epoch 00047: val_acc did not improve from 0.69524\n",
            "Epoch 48/100\n",
            "17/16 [==============================] - 8s 445ms/step - loss: 0.0307 - acc: 0.9926 - val_loss: 2.6478 - val_acc: 0.5048\n",
            "\n",
            "Epoch 00048: val_acc did not improve from 0.69524\n",
            "Epoch 49/100\n",
            "17/16 [==============================] - 8s 444ms/step - loss: 0.1134 - acc: 0.9703 - val_loss: 2.7967 - val_acc: 0.6095\n",
            "\n",
            "Epoch 00049: val_acc did not improve from 0.69524\n",
            "Epoch 50/100\n",
            "17/16 [==============================] - 8s 444ms/step - loss: 0.0479 - acc: 0.9853 - val_loss: 2.9231 - val_acc: 0.5048\n",
            "\n",
            "Epoch 00050: val_acc did not improve from 0.69524\n",
            "Epoch 51/100\n",
            "17/16 [==============================] - 8s 443ms/step - loss: 0.0725 - acc: 0.9754 - val_loss: 2.9574 - val_acc: 0.5524\n",
            "\n",
            "Epoch 00051: val_acc did not improve from 0.69524\n",
            "Epoch 52/100\n",
            "17/16 [==============================] - 7s 430ms/step - loss: 0.0856 - acc: 0.9753 - val_loss: 3.3090 - val_acc: 0.5048\n",
            "\n",
            "Epoch 00052: val_acc did not improve from 0.69524\n",
            "Epoch 53/100\n",
            "17/16 [==============================] - 8s 444ms/step - loss: 0.0257 - acc: 0.9945 - val_loss: 1.9539 - val_acc: 0.5905\n",
            "\n",
            "Epoch 00053: val_acc did not improve from 0.69524\n",
            "Epoch 54/100\n",
            "17/16 [==============================] - 8s 443ms/step - loss: 0.0761 - acc: 0.9735 - val_loss: 2.6632 - val_acc: 0.5619\n",
            "\n",
            "Epoch 00054: val_acc did not improve from 0.69524\n",
            "Epoch 55/100\n",
            "17/16 [==============================] - 8s 445ms/step - loss: 0.0420 - acc: 0.9853 - val_loss: 3.0018 - val_acc: 0.5143\n",
            "\n",
            "Epoch 00055: val_acc did not improve from 0.69524\n",
            "Epoch 56/100\n",
            "17/16 [==============================] - 7s 429ms/step - loss: 0.0565 - acc: 0.9809 - val_loss: 2.7331 - val_acc: 0.5429\n",
            "\n",
            "Epoch 00056: val_acc did not improve from 0.69524\n",
            "Epoch 57/100\n",
            "17/16 [==============================] - 8s 445ms/step - loss: 0.0361 - acc: 0.9908 - val_loss: 3.1001 - val_acc: 0.5333\n",
            "\n",
            "Epoch 00057: val_acc did not improve from 0.69524\n",
            "Epoch 58/100\n",
            "17/16 [==============================] - 8s 442ms/step - loss: 0.0364 - acc: 0.9889 - val_loss: 2.6303 - val_acc: 0.5524\n",
            "\n",
            "Epoch 00058: val_acc did not improve from 0.69524\n",
            "Epoch 59/100\n",
            "17/16 [==============================] - 8s 443ms/step - loss: 0.0500 - acc: 0.9853 - val_loss: 2.8132 - val_acc: 0.5714\n",
            "\n",
            "Epoch 00059: val_acc did not improve from 0.69524\n",
            "Epoch 60/100\n",
            "17/16 [==============================] - 8s 442ms/step - loss: 0.0231 - acc: 0.9926 - val_loss: 2.7300 - val_acc: 0.4952\n",
            "\n",
            "Epoch 00060: val_acc did not improve from 0.69524\n",
            "Epoch 61/100\n",
            "17/16 [==============================] - 7s 430ms/step - loss: 0.0479 - acc: 0.9871 - val_loss: 2.7009 - val_acc: 0.5714\n",
            "\n",
            "Epoch 00061: val_acc did not improve from 0.69524\n",
            "Epoch 62/100\n",
            "17/16 [==============================] - 8s 443ms/step - loss: 0.0604 - acc: 0.9834 - val_loss: 3.2040 - val_acc: 0.5143\n",
            "\n",
            "Epoch 00062: val_acc did not improve from 0.69524\n",
            "Epoch 63/100\n",
            "17/16 [==============================] - 8s 444ms/step - loss: 0.0110 - acc: 0.9963 - val_loss: 3.0174 - val_acc: 0.5238\n",
            "\n",
            "Epoch 00063: val_acc did not improve from 0.69524\n",
            "Epoch 64/100\n",
            "17/16 [==============================] - 8s 446ms/step - loss: 0.0264 - acc: 0.9926 - val_loss: 2.7670 - val_acc: 0.5714\n",
            "\n",
            "Epoch 00064: val_acc did not improve from 0.69524\n",
            "Epoch 65/100\n",
            "17/16 [==============================] - 8s 443ms/step - loss: 0.0528 - acc: 0.9853 - val_loss: 2.7483 - val_acc: 0.5714\n",
            "\n",
            "Epoch 00065: val_acc did not improve from 0.69524\n",
            "Epoch 66/100\n",
            "17/16 [==============================] - 7s 429ms/step - loss: 0.0647 - acc: 0.9864 - val_loss: 2.3864 - val_acc: 0.5524\n",
            "\n",
            "Epoch 00066: val_acc did not improve from 0.69524\n",
            "Epoch 67/100\n",
            "17/16 [==============================] - 8s 442ms/step - loss: 0.0384 - acc: 0.9871 - val_loss: 3.1772 - val_acc: 0.4857\n",
            "\n",
            "Epoch 00067: val_acc did not improve from 0.69524\n",
            "Epoch 68/100\n",
            "17/16 [==============================] - 8s 445ms/step - loss: 0.0567 - acc: 0.9809 - val_loss: 2.5818 - val_acc: 0.6000\n",
            "\n",
            "Epoch 00068: val_acc did not improve from 0.69524\n",
            "Epoch 69/100\n",
            "17/16 [==============================] - 8s 444ms/step - loss: 0.0264 - acc: 0.9908 - val_loss: 3.0388 - val_acc: 0.5238\n",
            "\n",
            "Epoch 00069: val_acc did not improve from 0.69524\n",
            "Epoch 70/100\n",
            "17/16 [==============================] - 7s 430ms/step - loss: 0.0960 - acc: 0.9728 - val_loss: 2.8486 - val_acc: 0.6000\n",
            "\n",
            "Epoch 00070: val_acc did not improve from 0.69524\n",
            "Epoch 71/100\n",
            "17/16 [==============================] - 8s 445ms/step - loss: 0.0225 - acc: 0.9889 - val_loss: 2.7880 - val_acc: 0.5524\n",
            "\n",
            "Epoch 00071: val_acc did not improve from 0.69524\n",
            "Epoch 72/100\n",
            "17/16 [==============================] - 8s 443ms/step - loss: 0.0625 - acc: 0.9779 - val_loss: 2.5104 - val_acc: 0.5905\n",
            "\n",
            "Epoch 00072: val_acc did not improve from 0.69524\n",
            "Epoch 73/100\n",
            "17/16 [==============================] - 8s 445ms/step - loss: 0.0240 - acc: 0.9908 - val_loss: 3.1460 - val_acc: 0.5524\n",
            "\n",
            "Epoch 00073: val_acc did not improve from 0.69524\n",
            "Epoch 74/100\n",
            "17/16 [==============================] - 8s 444ms/step - loss: 0.0478 - acc: 0.9853 - val_loss: 2.5723 - val_acc: 0.5619\n",
            "\n",
            "Epoch 00074: val_acc did not improve from 0.69524\n",
            "Epoch 75/100\n",
            "17/16 [==============================] - 7s 429ms/step - loss: 0.0235 - acc: 0.9908 - val_loss: 2.9489 - val_acc: 0.5714\n",
            "\n",
            "Epoch 00075: val_acc did not improve from 0.69524\n",
            "Epoch 76/100\n",
            "17/16 [==============================] - 7s 441ms/step - loss: 0.0510 - acc: 0.9926 - val_loss: 2.6786 - val_acc: 0.6190\n",
            "\n",
            "Epoch 00076: val_acc did not improve from 0.69524\n",
            "Epoch 77/100\n",
            "17/16 [==============================] - 8s 444ms/step - loss: 0.0472 - acc: 0.9926 - val_loss: 3.1652 - val_acc: 0.4857\n",
            "\n",
            "Epoch 00077: val_acc did not improve from 0.69524\n",
            "Epoch 78/100\n",
            "17/16 [==============================] - 8s 444ms/step - loss: 0.0701 - acc: 0.9742 - val_loss: 2.8376 - val_acc: 0.6190\n",
            "\n",
            "Epoch 00078: val_acc did not improve from 0.69524\n",
            "Epoch 79/100\n",
            "17/16 [==============================] - 8s 442ms/step - loss: 0.0100 - acc: 0.9963 - val_loss: 2.4469 - val_acc: 0.4952\n",
            "\n",
            "Epoch 00079: val_acc did not improve from 0.69524\n",
            "Epoch 80/100\n",
            "17/16 [==============================] - 7s 429ms/step - loss: 0.0275 - acc: 0.9926 - val_loss: 2.9999 - val_acc: 0.5524\n",
            "\n",
            "Epoch 00080: val_acc did not improve from 0.69524\n",
            "Epoch 81/100\n",
            "17/16 [==============================] - 8s 443ms/step - loss: 0.0562 - acc: 0.9797 - val_loss: 2.6016 - val_acc: 0.5524\n",
            "\n",
            "Epoch 00081: val_acc did not improve from 0.69524\n",
            "Epoch 82/100\n",
            "17/16 [==============================] - 8s 444ms/step - loss: 0.0785 - acc: 0.9834 - val_loss: 3.0649 - val_acc: 0.5429\n",
            "\n",
            "Epoch 00082: val_acc did not improve from 0.69524\n",
            "Epoch 83/100\n",
            "17/16 [==============================] - 8s 445ms/step - loss: 0.0539 - acc: 0.9816 - val_loss: 2.8094 - val_acc: 0.5810\n",
            "\n",
            "Epoch 00083: val_acc did not improve from 0.69524\n",
            "Epoch 84/100\n",
            "17/16 [==============================] - 7s 431ms/step - loss: 0.1589 - acc: 0.9673 - val_loss: 2.8996 - val_acc: 0.5333\n",
            "\n",
            "Epoch 00084: val_acc did not improve from 0.69524\n",
            "Epoch 85/100\n",
            "17/16 [==============================] - 8s 444ms/step - loss: 0.0469 - acc: 0.9791 - val_loss: 2.3255 - val_acc: 0.5619\n",
            "\n",
            "Epoch 00085: val_acc did not improve from 0.69524\n",
            "Epoch 86/100\n",
            "17/16 [==============================] - 8s 444ms/step - loss: 0.0853 - acc: 0.9772 - val_loss: 3.2238 - val_acc: 0.5143\n",
            "\n",
            "Epoch 00086: val_acc did not improve from 0.69524\n",
            "Epoch 87/100\n",
            "17/16 [==============================] - 8s 444ms/step - loss: 0.2036 - acc: 0.9655 - val_loss: 2.6320 - val_acc: 0.5905\n",
            "\n",
            "Epoch 00087: val_acc did not improve from 0.69524\n",
            "Epoch 88/100\n",
            "17/16 [==============================] - 8s 445ms/step - loss: 0.0329 - acc: 0.9889 - val_loss: 2.9639 - val_acc: 0.5333\n",
            "\n",
            "Epoch 00088: val_acc did not improve from 0.69524\n",
            "Epoch 89/100\n",
            "17/16 [==============================] - 7s 429ms/step - loss: 0.0351 - acc: 0.9908 - val_loss: 2.9227 - val_acc: 0.5238\n",
            "\n",
            "Epoch 00089: val_acc did not improve from 0.69524\n",
            "Epoch 90/100\n",
            "17/16 [==============================] - 8s 443ms/step - loss: 0.1832 - acc: 0.9593 - val_loss: 1.9458 - val_acc: 0.6000\n",
            "\n",
            "Epoch 00090: val_acc did not improve from 0.69524\n",
            "Epoch 91/100\n",
            "17/16 [==============================] - 8s 447ms/step - loss: 0.0703 - acc: 0.9747 - val_loss: 3.6936 - val_acc: 0.5238\n",
            "\n",
            "Epoch 00091: val_acc did not improve from 0.69524\n",
            "Epoch 92/100\n",
            "17/16 [==============================] - 8s 444ms/step - loss: 0.0719 - acc: 0.9760 - val_loss: 2.2004 - val_acc: 0.5810\n",
            "\n",
            "Epoch 00092: val_acc did not improve from 0.69524\n",
            "Epoch 93/100\n",
            "17/16 [==============================] - 8s 443ms/step - loss: 0.0275 - acc: 0.9926 - val_loss: 3.1290 - val_acc: 0.5333\n",
            "\n",
            "Epoch 00093: val_acc did not improve from 0.69524\n",
            "Epoch 94/100\n",
            "17/16 [==============================] - 7s 430ms/step - loss: 0.1426 - acc: 0.9747 - val_loss: 2.3956 - val_acc: 0.5619\n",
            "\n",
            "Epoch 00094: val_acc did not improve from 0.69524\n",
            "Epoch 95/100\n",
            "17/16 [==============================] - 8s 442ms/step - loss: 0.2080 - acc: 0.9747 - val_loss: 2.9732 - val_acc: 0.5429\n",
            "\n",
            "Epoch 00095: val_acc did not improve from 0.69524\n",
            "Epoch 96/100\n",
            "17/16 [==============================] - 8s 444ms/step - loss: 0.1668 - acc: 0.9809 - val_loss: 3.2869 - val_acc: 0.5143\n",
            "\n",
            "Epoch 00096: val_acc did not improve from 0.69524\n",
            "Epoch 97/100\n",
            "17/16 [==============================] - 8s 442ms/step - loss: 0.0378 - acc: 0.9853 - val_loss: 2.4636 - val_acc: 0.5810\n",
            "\n",
            "Epoch 00097: val_acc did not improve from 0.69524\n",
            "Epoch 98/100\n",
            "17/16 [==============================] - 7s 431ms/step - loss: 0.0446 - acc: 0.9852 - val_loss: 2.8271 - val_acc: 0.5048\n",
            "\n",
            "Epoch 00098: val_acc did not improve from 0.69524\n",
            "Epoch 99/100\n",
            "17/16 [==============================] - 8s 442ms/step - loss: 0.0275 - acc: 0.9908 - val_loss: 2.8919 - val_acc: 0.5905\n",
            "\n",
            "Epoch 00099: val_acc did not improve from 0.69524\n",
            "Epoch 100/100\n",
            "17/16 [==============================] - 8s 443ms/step - loss: 0.0411 - acc: 0.9871 - val_loss: 2.5996 - val_acc: 0.5524\n",
            "\n",
            "Epoch 00100: val_acc did not improve from 0.69524\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LKESRTjg4spa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Carregando o modelo com o peso melhor treinado (exemplo: carregando epoch 47, validation accuracy de 90.5%)\n",
        "\n",
        "model.load_weights('/content/drive/My Drive/Colab Notebooks/log/inceptionv3:009-val_acc:0.695.hdf5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6voWAktx5Bcq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 573
        },
        "outputId": "3d50571b-1bb9-4e78-8666-59b2d637df37"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# GrÃ¡fico de treino - acurÃ¡cia\n",
        "plt.plot(history.history['acc'])\n",
        "plt.plot(history.history['val_acc'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\n",
        "# GrÃ¡fico de treino - perda\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEWCAYAAABliCz2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUax1d6r-bhI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "2a94bdb4-226b-45e8-f415-4eea4aa061be"
      },
      "source": [
        "# mÃ©tricas de perda e acurÃ¡cia\n",
        "score = model.evaluate_generator(validation_set, 105)\n",
        "\n",
        "print (\"%s: %.2f%%\" % (model.metrics_names[0], score[0]*100))\n",
        "print (\"%s: %.2f%%\" % (model.metrics_names[1], score[1]*100))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loss: 302.82%\n",
            "acc: 61.96%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yw8-bjXWEb_9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "a6df7608-7153-4dfb-8589-a8832b5e2481"
      },
      "source": [
        "!pip install --upgrade --quiet PyDrive\n",
        "# para conectar com o Google Drive\n",
        "\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[?25l\r\u001b[K     |â                               | 10kB 17.4MB/s eta 0:00:01\r\u001b[K     |â                               | 20kB 1.8MB/s eta 0:00:01\r\u001b[K     |â                               | 30kB 2.7MB/s eta 0:00:01\r\u001b[K     |ââ                              | 40kB 1.7MB/s eta 0:00:01\r\u001b[K     |ââ                              | 51kB 2.1MB/s eta 0:00:01\r\u001b[K     |ââ                              | 61kB 2.5MB/s eta 0:00:01\r\u001b[K     |âââ                             | 71kB 2.9MB/s eta 0:00:01\r\u001b[K     |âââ                             | 81kB 3.3MB/s eta 0:00:01\r\u001b[K     |âââ                             | 92kB 3.7MB/s eta 0:00:01\r\u001b[K     |ââââ                            | 102kB 2.8MB/s eta 0:00:01\r\u001b[K     |ââââ                            | 112kB 2.8MB/s eta 0:00:01\r\u001b[K     |ââââ                            | 122kB 2.8MB/s eta 0:00:01\r\u001b[K     |âââââ                           | 133kB 2.8MB/s eta 0:00:01\r\u001b[K     |âââââ                           | 143kB 2.8MB/s eta 0:00:01\r\u001b[K     |âââââ                           | 153kB 2.8MB/s eta 0:00:01\r\u001b[K     |ââââââ                          | 163kB 2.8MB/s eta 0:00:01\r\u001b[K     |ââââââ                          | 174kB 2.8MB/s eta 0:00:01\r\u001b[K     |ââââââ                          | 184kB 2.8MB/s eta 0:00:01\r\u001b[K     |âââââââ                         | 194kB 2.8MB/s eta 0:00:01\r\u001b[K     |âââââââ                         | 204kB 2.8MB/s eta 0:00:01\r\u001b[K     |âââââââ                         | 215kB 2.8MB/s eta 0:00:01\r\u001b[K     |ââââââââ                        | 225kB 2.8MB/s eta 0:00:01\r\u001b[K     |ââââââââ                        | 235kB 2.8MB/s eta 0:00:01\r\u001b[K     |ââââââââ                        | 245kB 2.8MB/s eta 0:00:01\r\u001b[K     |âââââââââ                       | 256kB 2.8MB/s eta 0:00:01\r\u001b[K     |âââââââââ                       | 266kB 2.8MB/s eta 0:00:01\r\u001b[K     |âââââââââ                       | 276kB 2.8MB/s eta 0:00:01\r\u001b[K     |ââââââââââ                      | 286kB 2.8MB/s eta 0:00:01\r\u001b[K     |ââââââââââ                      | 296kB 2.8MB/s eta 0:00:01\r\u001b[K     |ââââââââââ                      | 307kB 2.8MB/s eta 0:00:01\r\u001b[K     |âââââââââââ                     | 317kB 2.8MB/s eta 0:00:01\r\u001b[K     |âââââââââââ                     | 327kB 2.8MB/s eta 0:00:01\r\u001b[K     |âââââââââââ                     | 337kB 2.8MB/s eta 0:00:01\r\u001b[K     |ââââââââââââ                    | 348kB 2.8MB/s eta 0:00:01\r\u001b[K     |ââââââââââââ                    | 358kB 2.8MB/s eta 0:00:01\r\u001b[K     |ââââââââââââ                    | 368kB 2.8MB/s eta 0:00:01\r\u001b[K     |âââââââââââââ                   | 378kB 2.8MB/s eta 0:00:01\r\u001b[K     |âââââââââââââ                   | 389kB 2.8MB/s eta 0:00:01\r\u001b[K     |âââââââââââââ                   | 399kB 2.8MB/s eta 0:00:01\r\u001b[K     |ââââââââââââââ                  | 409kB 2.8MB/s eta 0:00:01\r\u001b[K     |ââââââââââââââ                  | 419kB 2.8MB/s eta 0:00:01\r\u001b[K     |ââââââââââââââ                  | 430kB 2.8MB/s eta 0:00:01\r\u001b[K     |âââââââââââââââ                 | 440kB 2.8MB/s eta 0:00:01\r\u001b[K     |âââââââââââââââ                 | 450kB 2.8MB/s eta 0:00:01\r\u001b[K     |âââââââââââââââ                 | 460kB 2.8MB/s eta 0:00:01\r\u001b[K     |ââââââââââââââââ                | 471kB 2.8MB/s eta 0:00:01\r\u001b[K     |ââââââââââââââââ                | 481kB 2.8MB/s eta 0:00:01\r\u001b[K     |ââââââââââââââââ                | 491kB 2.8MB/s eta 0:00:01\r\u001b[K     |âââââââââââââââââ               | 501kB 2.8MB/s eta 0:00:01\r\u001b[K     |âââââââââââââââââ               | 512kB 2.8MB/s eta 0:00:01\r\u001b[K     |âââââââââââââââââ               | 522kB 2.8MB/s eta 0:00:01\r\u001b[K     |ââââââââââââââââââ              | 532kB 2.8MB/s eta 0:00:01\r\u001b[K     |ââââââââââââââââââ              | 542kB 2.8MB/s eta 0:00:01\r\u001b[K     |ââââââââââââââââââ              | 552kB 2.8MB/s eta 0:00:01\r\u001b[K     |âââââââââââââââââââ             | 563kB 2.8MB/s eta 0:00:01\r\u001b[K     |âââââââââââââââââââ             | 573kB 2.8MB/s eta 0:00:01\r\u001b[K     |âââââââââââââââââââ             | 583kB 2.8MB/s eta 0:00:01\r\u001b[K     |ââââââââââââââââââââ            | 593kB 2.8MB/s eta 0:00:01\r\u001b[K     |ââââââââââââââââââââ            | 604kB 2.8MB/s eta 0:00:01\r\u001b[K     |ââââââââââââââââââââ            | 614kB 2.8MB/s eta 0:00:01\r\u001b[K     |âââââââââââââââââââââ           | 624kB 2.8MB/s eta 0:00:01\r\u001b[K     |âââââââââââââââââââââ           | 634kB 2.8MB/s eta 0:00:01\r\u001b[K     |âââââââââââââââââââââ           | 645kB 2.8MB/s eta 0:00:01\r\u001b[K     |ââââââââââââââââââââââ          | 655kB 2.8MB/s eta 0:00:01\r\u001b[K     |ââââââââââââââââââââââ          | 665kB 2.8MB/s eta 0:00:01\r\u001b[K     |ââââââââââââââââââââââ          | 675kB 2.8MB/s eta 0:00:01\r\u001b[K     |âââââââââââââââââââââââ         | 686kB 2.8MB/s eta 0:00:01\r\u001b[K     |âââââââââââââââââââââââ         | 696kB 2.8MB/s eta 0:00:01\r\u001b[K     |âââââââââââââââââââââââ         | 706kB 2.8MB/s eta 0:00:01\r\u001b[K     |ââââââââââââââââââââââââ        | 716kB 2.8MB/s eta 0:00:01\r\u001b[K     |ââââââââââââââââââââââââ        | 727kB 2.8MB/s eta 0:00:01\r\u001b[K     |ââââââââââââââââââââââââ        | 737kB 2.8MB/s eta 0:00:01\r\u001b[K     |âââââââââââââââââââââââââ       | 747kB 2.8MB/s eta 0:00:01\r\u001b[K     |âââââââââââââââââââââââââ       | 757kB 2.8MB/s eta 0:00:01\r\u001b[K     |âââââââââââââââââââââââââ       | 768kB 2.8MB/s eta 0:00:01\r\u001b[K     |ââââââââââââââââââââââââââ      | 778kB 2.8MB/s eta 0:00:01\r\u001b[K     |ââââââââââââââââââââââââââ      | 788kB 2.8MB/s eta 0:00:01\r\u001b[K     |ââââââââââââââââââââââââââ      | 798kB 2.8MB/s eta 0:00:01\r\u001b[K     |âââââââââââââââââââââââââââ     | 808kB 2.8MB/s eta 0:00:01\r\u001b[K     |âââââââââââââââââââââââââââ     | 819kB 2.8MB/s eta 0:00:01\r\u001b[K     |âââââââââââââââââââââââââââ     | 829kB 2.8MB/s eta 0:00:01\r\u001b[K     |ââââââââââââââââââââââââââââ    | 839kB 2.8MB/s eta 0:00:01\r\u001b[K     |ââââââââââââââââââââââââââââ    | 849kB 2.8MB/s eta 0:00:01\r\u001b[K     |ââââââââââââââââââââââââââââ    | 860kB 2.8MB/s eta 0:00:01\r\u001b[K     |âââââââââââââââââââââââââââââ   | 870kB 2.8MB/s eta 0:00:01\r\u001b[K     |âââââââââââââââââââââââââââââ   | 880kB 2.8MB/s eta 0:00:01\r\u001b[K     |âââââââââââââââââââââââââââââ   | 890kB 2.8MB/s eta 0:00:01\r\u001b[K     |ââââââââââââââââââââââââââââââ  | 901kB 2.8MB/s eta 0:00:01\r\u001b[K     |ââââââââââââââââââââââââââââââ  | 911kB 2.8MB/s eta 0:00:01\r\u001b[K     |ââââââââââââââââââââââââââââââ  | 921kB 2.8MB/s eta 0:00:01\r\u001b[K     |âââââââââââââââââââââââââââââââ | 931kB 2.8MB/s eta 0:00:01\r\u001b[K     |âââââââââââââââââââââââââââââââ | 942kB 2.8MB/s eta 0:00:01\r\u001b[K     |âââââââââââââââââââââââââââââââ | 952kB 2.8MB/s eta 0:00:01\r\u001b[K     |ââââââââââââââââââââââââââââââââ| 962kB 2.8MB/s eta 0:00:01\r\u001b[K     |ââââââââââââââââââââââââââââââââ| 972kB 2.8MB/s eta 0:00:01\r\u001b[K     |ââââââââââââââââââââââââââââââââ| 983kB 2.8MB/s eta 0:00:01\r\u001b[K     |ââââââââââââââââââââââââââââââââ| 993kB 2.8MB/s \n",
            "\u001b[?25h  Building wheel for PyDrive (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HZNqOp_SEamw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Salvando o modelo no drive\n",
        "\n",
        "model.save(\"model_inceptionv3.h5\")\n",
        "uploaded = drive.CreateFile({'title': 'model_inceptionv3.h5'})\n",
        "uploaded.SetContentFile('model_inceptionv3.h5')\n",
        "uploaded.Upload()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}